---
title: "Programming Historian on YouTube Comments in Wordfish"
author: "Jeff Antsen, Nicole Lemire-Garlic, Alex Wermer-Colan"
date: "6/3/2020"
output:
  html_document: default
  pdf_document: default
---

# SETUP
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(eval=TRUE)
#knitr::opts_root.dir$.... #@ there is syntax for correctly setting a directory alternate to the base one here.
#@ I don't think we even need to worry about that, though.

```

### Install Packages 
```{r install_packages, eval=F}
# Install Packages and Library Calls
#install.packages("knitr")
#install.packages("NLP")
#install.packages("tm")
#install.packages("tidyverse")
#install.packages("ggplot2")
#install.packages("austin", repos="http://R-Forge.R-project.org")
#install.packages("beepr")
#install.packages("RColorBrewer")
#install.packages("tuber")
#install.packages("gtools")
#install.packages("data.table")
#install.packages("lubridate")
```

### Call Libraries
```{r call_libraries, echo=F, results='hide'}
#library(kintr)
library(NLP)
library(tm)
library(tidyverse)
library(ggplot2)
library(austin)
library(beepr)
library(RColorBrewer)
library(tuber)
library(gtools)
library(data.table)
library(lubridate)
```

### Set Parameters / Tolerances
```{r Parameters_and_Tolerances, echo=F, results='hide'}
subject_s <- "COVID"  #@ what the project is about
SearchTerms <- c("reopening america", "reopen america", "#reopenamerica", "freeamericanow")
#@ these are the keyword terms which will be used to identify videos
```

# DATA and Directories
```{r create_directories, echo=F, results='hide'}
input.dir <- "Data/"
proj.dir <- "C:/Users/alwer/Documents/GitHub/Programming-Historian-Tutorial"
setwd(proj.dir)
getwd()
```

### Create(scrape) comment data object
```{r scrape_comments, echo=T, results='hide'}
#saving the client ID and client secret for tuber package
#at prompt choose "1: Yes"
source("Config.R")
app_id <- API_ID
app_secret <- API_Secret
yt_oauth(app_id, app_secret)

#type in list of videos IDS
video_list <- c("mM45rpHMZD4", "WsUBnOv70Mw", "qzrw8-TZN_A", "KxtGJsnLgSc", "aPLEJKluMgU")

#get_all_comments(video_id = video_list[1])
stats <- get_stats("mM45rpHMZD4")
get_avlbl_comments <- possibly(get_all_comments, otherwise = NULL)
AllComments <- map(video_list, get_avlbl_comments)

#reduce possibly output (which is a list) to a dataframe
AllCommentsdf <- AllComments %>% reduce(full_join)

### piping in data from the other script
#AllComments_df <- AllCommentsMetadata_aj (THROWS ERROR)

#displays which videos were scraped
AllCommentsVideos <- unique(AllCommentsdf$videoId)
AllCommentsVideos

#output comments
write.csv(AllComments_df, paste("AllComments_", subject_s, ".csv", sep=""))
```

#Tokenize 'elaborate' (long + multi-type) comments
```{r tokenize_elaborate_comments, echo=F, results='hide'}
#Creates list of tokenized comment vectors, used to make corpus object
comment_tokens_l <- list()

for (com in 1:nrow(AllComments_df)) {
  sto <- NA
  sto <- AllComments_df$textOriginal[com]
  #@ preprocess sto  
  sto.list <- list()
  sto <- tolower(sto)
  sto.list <- strsplit(sto, "\\W")
  sto.text <- unlist(sto.list)
  sto_rm <- sto.text[-which(sto.text == "")]; sto.text <- sto_rm
  if(length (sto.text) >= 40 & length(unique(sto.text)) >=15 ) {
    #@ create list.head               
    list.head <- NA
    list.head <- AllComments_df$id[com]
    #list.head <- substring(list.head, 1, nchar(list.head)-20)  (subset the comment title string if/as desired)
    comment_tokens_l[[list.head]] <- sto.text
  
  }}
print(paste("keeping",length(comment_tokens_l), "out of", nrow(AllComments_df),"comments"))

list_names <- names(comment_tokens_l)
```

### Get Ready to Wordfish!
```{r data_reshape_for_WF, echo=FALSE, warning=FALSE, results='hide'}
####### Turn the list of comment files into a corpus data object
comment_corpus <- Corpus(VectorSource(comment_tokens_l)) 

#Clean corpus
comment_corpus <- tm_map(comment_corpus, removeNumbers)
comment_corpus <- tm_map(comment_corpus, removePunctuation)

#Remove stop words
my_stop <- c(stopwords("english"),
             "c","x", "s", "t", "m", "amp", "youtube", "www", "com", "quot", "br", "http", "https", "")
comment_corpus <- tm_map(comment_corpus, removeWords, my_stop)

#DocumentTermMatrix
dtm = DocumentTermMatrix(comment_corpus)
```


### Control Sparseness and Run the Wordfish model
```{r WF_model, echo=TRUE, results='hide'}
##Set accepted sparsity parameter
dtma = removeSparseTerms(dtm, sparse = .998)

print(paste("using",length(dtma$dimnames$Terms),"words from", 
            length(dtma$dimnames$Docs), "of", nrow(AllComments_df), "comments... trying to WF!"))


# Running a wordfish model
wfa1 <- wordfish(as.wfm(dtma), dir=c(1, 2), control = list(tol = 3.0e-5), verbose = T)
```

### Store data from WF model
```{r store_WF_model_data, echo=FALSE}
# Explore Wordfish 
wfa1$docs <- names(comment_tokens_l)
wfdocs.v <- names(comment_tokens_l)

wfdocs.v <- wfa1[["docs"]]
theta <- wfa1[["theta"]]
alpha <- wfa1[["alpha"]]

#@ the [-1] indexing for these removes the empty strings ("") that are sometimes an artifact of this method
#@ you SHOULD NOT need this
(head(wfa1$words))

wf.words.v <- wfa1[["words"]];    #wf.words.v <- wf.words.v[-1]
beta <- wfa1[["beta"]];           #beta<-beta[-1]
psi <- wfa1[["psi"]];             #psi <- psi [-1]
```

### View some properties of the WF model
```{r view_WF_model_properties, echo=TRUE}
sum(theta[which(theta>0)])
sum(theta[which(theta<0)])
mean(theta)
sum(theta)

### View a histogram of the distribution of each key variable
hist(theta, breaks=30)  # document polarity (refined iteratively)
hist(alpha, breaks=30)  # fixed effect for document length
hist(beta, breaks=50)   # word polarity (refined iteratively)
hist(psi, breaks=50)    # fixed effect for term (aka ~type~) frequency

```

### Make Composite Data Objects
```{r make_WF_data_objects, echo=FALSE, results='hide'}
#Make Composite data objects

source <- NA
for (i in 1:length(wfdocs.v)){
  sto<-NA
  sto <- gsub("[0-9]", "", wfdocs.v[i])
  sto <- gsub(".txt", "", sto)
  sto <- gsub("article","",sto)
  sto <- gsub("[:.:]","",sto)
  source[i] <- sto
}
##  TITLE is the title with metadata stripped out 
title<-NA
for (i in 1:length(wfdocs.v)){
  sto<-NA
  sto <- substr(wfdocs.v[i],1,12 )
  title[i] <- sto
}

### CREATE DOC DATA DATAFRAME
wf_docdata.df <- data.frame(wfdocs.v, source, title, theta, alpha)

### CREATE WORD DATA DATAFRAME
wf_worddata.df <- data.frame(wf.words.v, beta, psi)
```

### Visualizations Directory
```{r save_vis_in_directory, echo=FALSE, eval=FALSE}
### create and set a visualization directory
vis.dir <- "/Visualizations/"
dir.create("Visualizations")
working.vis.dir <- paste(proj.dir, vis.dir, sep="" )
working.vis.dir
setwd(working.vis.dir)

subject.dir <- paste(subject_s, ".vis", "/", sep="")
dir.create(subject.dir)
setwd(paste(data.dir, vis.dir, subject.dir, sep=""))
getwd()
```


### Initial Visualizations
```{r initial_visualizations, echo=FALSE}
## Plot the two estimated document parameters: THETA against ALPHA, 
#@ Alpha = document fixed effect (control for length)
#@ Theta = document "polarity" - alignment along as-identified latent principle component

###### Label by USER color by CLIQUE can include color=source in aes
user_T_A_plot <- ggplot(data = wf_docdata.df, mapping = aes(x =theta, y = alpha, label = title)) + 
  geom_text(size = 1.2) + 
  labs(x = "Comment polarity: an optimized value (theta)", y = "Comment length: a fixed effect (alpha)") +
  #guides(size = "none", color = guide_legend("")) + theme(legend.position="bottom") +
  labs(title = "Comment Polarity in comment network cliques of one thread scraped from YouTube", 
                      subtitle="'CNN reporter presses Trump: You promised Mexico would pay for wall'") 
user_T_A_plot 


ggsave(paste(subject_s, "SUBSET_user_T_A_plot.pdf",sep=""), device="pdf")

###### COLORLESS
source_T_A_plot <- ggplot(data = wf_docdata.df, mapping = aes(x =theta, y = alpha, label = title)) + 
  geom_text(size = 1) + 
  labs(x = "Doc polarity: an optimized value (theta)", y = "Doc length: a fixed effect (alpha)") +
  #guides(size = "none", color = guide_legend("")) + theme(legend.position="bottom") +
  labs(title = paste (subject_s, 
                      " comments from ABC YouTube Videos:\n Article IDs plotted, shaded by comment thread source", sep="")) 
##     __ 
source_T_A_plot

#ggsave(paste(subject.s, "SUBSET_colorless_T_A_plot.pdf",sep=""), device="pdf")

###FIRST PLOT of two word parameters: BETA against PSI, basic black and white
#@***limiting by a critical max negitive psi value***
word_P_B_plot <- ggplot(data = wf_worddata.df, mapping = aes(x = beta, y = psi, label = wf.words.v)) + 
  geom_text(data=subset(wf_worddata.df, psi>-20), size = 0.755) + 
  labs(x = "Word polarity: an optimized value (beta)", y = "Word frequency: a fixed effect (psi)") +
  #guides(size = "none", color = guide_legend("")) + 
  labs(title = "Vocabulary Polarity in comment network cliques of one thread scraped from YouTube", 
       subtitle="'CNN reporter presses Trump: You promised Mexico would pay for wall'")
######           __
#word_P_B_plot
getwd()

ggsave(paste(subject_s, "Word_P_B_plot.pdf",sep=""), device="pdf")

```

### Colorizing Key Words
```{r give_keywords_color, echo=FALSE}
neutral <- "neutral"  # Grey
topA <- "Of Interest"      # Red
topB <- "Wall"      # 
topC <- "Money"      # 
topD <- "Trade"      # 
topE <- "Rape"     # 
topF <- "Voting"   # 
topG <- "Drugs"    # 
topH <- "Jobs"     # 
#topic_colors <- c()

wf_worddata.df$key <- neutral  ### SET / RESET default font SIZE/COLOR and word coding

ktA <- c("trump", "animal", "animals", "obama", "illegal", "mexico", "mexican", "caravan", 
         "jew", "jews", "jewish", "security", "secure", "national", "nation")
for(k in 1:length(ktA)){
  sto <- NA
  sto <- (which(word==ktA[k]))
  #print(sto)
  wf_worddata.df$key[sto] <-topA }   ### NEW FONT SIZE
#View(wf_worddata.df[which(wf_worddata.df$key==topA),])

ktB <- c("wall", "build", "steel", "concrete", "border", "dig", "tunnels", "bars", "invisible", "see")
for(k in 1:length(ktB)){
  sto <- NA
  sto <- (which(word==ktB[k]))
  wf_worddata.df$key[sto] <-topB }    ### NEW FONT SIZE
#View(wf_worddata.df[which(wf_worddata.df$key==topB),])

ktC <- c("money", "tax", "taxes", "taxpayers", "billion", "billions", "spend", "spent", "pay", "billionz")
for(k in 1:length(ktC)){
  sto <- NA
  sto <- (which(word==ktC[k]))
  wf_worddata.df$key[sto] <-topC }    ### NEW FONT SIZE
#View(wf_worddata.df[which(wf_worddata.df$key==topC),])

ktD <- c("nafta", "trade", "usmca")
for(k in 1:length(ktD)){
  sto <- NA
  sto <- (which(word==ktD[k]))
  wf_worddata.df$key[sto] <-topD }    ### NEW FONT SIZE
#View(wf_worddata.df[which(wf_worddata.df$key==topD),])


ktE <- c("rape", "raped", "raping", "rapes", "assault", "sexual", "victim", "rapists", "ptsd")
for(k in 1:length(ktE)){
  sto <- NA
  sto <- (which(word==ktE[k]))
  wf_worddata.df$key[sto] <-topE }    ### NEW FONT SIZE
#View(wf_worddata.df[which(wf_worddata.df$key==topE),])

ktF <- c("vote", "voting", "election", "elections", "electoral", "rights", "birthrights")
for(k in 1:length(ktF)){
  sto <- NA
  sto <- (which(word==ktF[k]))
  wf_worddata.df$key[sto] <-topF }    ### NEW FONT SIZE
#View(wf_worddata.df[which(wf_worddata.df$key==topF),])

ktG <- c("drug", "drugs", "illicit", "police", "marijuana", "heroin", "coke", "cocaine", "fentanyl")
for(k in 1:length(ktG)){
  sto <- NA
  sto <- (which(word==ktG[k]))
  wf_worddata.df$key[sto] <-topG }    ### NEW FONT SIZE
#View(wf_worddata.df[which(wf_worddata.df$key==topG),])


ktH <- c("job", "jobs", "economic", "work", "worker", "labor", "laborer", "farm", "farms", "agriculture")
for(k in 1:length(ktH)){
  sto <- NA
  sto <- (which(word==ktH[k]))
  wf_worddata.df$key[sto] <-topH }    ### NEW FONT SIZE
#View(wf_worddata.df[which(wf_worddata.df$key==topH),])

```

### Final Colorized-by-words plot
```{r word_color_vis, echo=FALSE, warning=FALSE}
#+++++++++++++++  use this step to snip off extreem beta and psi vales to maximize plotting area
wordLegend_P_B_plot <- ggplot(data = wf_worddata.df, mapping = aes(x = beta, y = psi, label = word, color=key)) + 
  ylim(-11,.2)+ xlim(-5,6) +       #tight limits
  #ylim(-14,.2)+ xlim(-7,8) +     #more expanded limits
  geom_text(data=subset(wf_worddata.df, key== neutral), size = .85, color="gray") + 
  geom_text(data=subset(wf_worddata.df, key!= neutral), size = 2.2) +
  scale_color_discrete(l=40) +
  #scale_color_manual(values=c("#00008B", "#8B2323", "#006400", "goldenrod4", "#8B0A50" )) +
  guides(size = "none", color = guide_legend("")) + theme(legend.position = "top") +
  labs(x = "Word polarity: an optimized value (beta)", y = "Word frequency: a fixed effect (psi)") +
  labs(title = "Vocabulary Polarity in comment network cliques of one thread scraped from YouTube", 
       subtitle="'CNN reporter presses Trump: You promised Mexico would pay for wall'\n key terms bolded")

#wordLegend_P_B_plot
```