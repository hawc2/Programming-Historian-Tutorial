---
title: "Programming Historian on YouTube Comments in Wordfish"
author: "Jeff Antsen, Nicole Lemire-Garlic, Alex Wermer-Colan"
date: "6/3/2020"
output:
  html_document: default
  pdf_document: default
---

# SETUP
### Set code block default prefs
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(eval=TRUE)
#knitr::opts_root.dir$.... #@ there is syntax for correctly setting a directory alternate to the base one here.
#@ I don't think we even need to worry about that, though.

```

### Install Packages 
```{r install_packages, eval=F}
# Install Packages and Library Calls
#install.packages("knitr")
#install.packages("NLP")
#install.packages("tm")
#install.packages("tidyverse")
#install.packages("ggplot2")
#install.packages("austin", repos="http://R-Forge.R-project.org")
#install.packages("beepr")
#install.packages("RColorBrewer")
#install.packages("tuber")
#install.packages("gtools")
#install.packages("data.table")
#install.packages("lubridate")
```

### Call Libraries
```{r call_libraries, echo=F, results='hide'}
#library(kintr)
library(NLP)
library(tm)
library(tidyverse)
library(ggplot2)
library(austin)
library(beepr)
library(RColorBrewer)
library(tuber)
library(gtools)
library(data.table)
library(lubridate)
```

### Set Parameters / Tolerances
~~ Do we want to do this here, or in the Config file?  Might make more sense there...?
```{r Parameters_and_Tolerances, echo=F, results='hide'}
# CLEANUP your workspace
##       __       rm(list=ls())


subject_s <- "COVID_and_Opening_America"  #@ what the project is about (this will appear in visualization titles and file names)
SearchTerms <- c("reopening america", "reopen america", "#reopenamerica", "freeamericanow", "#freeamericanow")
#@ these are the keyword terms which will be used to identify videos


#### (are there other video level parameters?)

### (comment-level parameters?)
# How many characters do you want your comments to be at minimum, initially?
# How many tokens (minimum) do you want your final comments to have post-stopwords and sparsity?


### Any other relevant up-front parameters?

```

### Authorize your API account
Remember to push 1 for 'yes; in the CONSOLE! And then log into your gmail/google account.
```{r authorize, echo=T, results='hide'}
#Saving the client ID and client secret for subsequent use by the tuber package
#at prompt choose "1: Yes IN THE CONSOLE!"
#this creates your personal .httr-oauth file - you only need to do this once, as long as you don't delete that file!
source("Config.R")

app_id <- API_ID
app_secret <- API_Secret
yt_oauth(app_id, app_secret)
```



# 'CODE' begins here
### Get list of videos matching your search terms
and organize that video list
```{r get_video_list}
SearchResults <- map(SearchTerms, yt_search)
SearchResultsDF <- do.call(rbind, lapply(SearchResults, data.frame))
SearchResultsDF[] <- lapply(SearchResultsDF, as.character)
SearchResultsDF$publishedAt <- SearchResultsDF$publishedAt %>% as_datetime(tz = "UTC", format = NULL)
SearchResultsDF <- select(SearchResultsDF, video_id, publishedAt, title, channelTitle) %>% arrange(desc(publishedAt))

#filter results by date in RFC339 format  (???)
SearchResultsDF <- SearchResultsDF %>% filter(publishedAt > "2020-03-01T00:00:00Z")
#final list of videos
video_list <-as.vector(SearchResultsDF$video_id)
```

### Scrape available comments from those videos
```{r scrape_available_comments}
#gets video comments from API and converts to dataframe
#uses possibly to avoid error messages for unavailable comments
get_avlbl_comments <- possibly(get_all_comments, otherwise = NULL)
AllComments <- map(video_list, get_avlbl_comments)
AllCommentsDF <- do.call(rbind, lapply(AllComments, data.frame))
#View(AllCommentsDF)

#displays which video comments were scraped 
AllCommentsVideos <- unique(AllCommentsDF$videoId)


print(paste("You have identified", length(AllCommentsVideos), "unique videos with: ", paste(SearchTerms, collapse=" "),"!"))
```


### Scrape Metadata (includes video titles)
```{r scrape_and_join_metadata}
#joins video metadata to comments and renames columns for clarity
AllCommentsMetadata <- inner_join(AllCommentsDF, SearchResultsDF, by = c("videoId" = "video_id"))
AllCommentsMetadata <- rename(AllCommentsMetadata, c(commentPublishedAt = publishedAt.x, 
                                                     commentUpdatedAt = updatedAt,
                                                     commentLikeCount = likeCount,
                                                     commentId = id,
                                                     videoPublishedAt= publishedAt.y,
                                                     videoTitle = title,
                                                     videoChannelTitle = channelTitle))
#COMMENTS <- AllCommentsMetadata


#confirm videos scraped and number of comments per video
totalScraped <- AllCommentsMetadata %>% group_by(videoId) %>% tally() %>% arrange(desc(n))
View(totalScraped)


#remove videos with less than 100 comments and view final count
tooFew <- filter(totalScraped, n < 100)
AllCommentsMetadata_aj <- AllCommentsMetadata %>% anti_join(tooFew)
AllCommentsMetadata_aj %>% group_by(videoId) %>% tally() %>% arrange(desc(n))

#output comments and metadata
#write.csv(AllCommentsMetadata, "AllCommentsMetadata2.csv")

```

### See Comments
```{r see_which_comments, echo=T}
#displays which videos were scraped
AllCommentsVideos <- unique(AllCommentsDF$videoId)
#View(AllCommentsVideos)
```

### Save Comment Data
```{r output_comments, eval=F, echo=F}
#output comments
write.csv(AllComments_df, paste("AllComments_", subject.s, ".csv", sep=""))
```



#Tokenize 'elaborate' (long + multi-type) comments
```{r tokenize_elaborate_comments, echo=F, results='hide'}

###################### This creates list of tokenized comment vectors, used to make the corpus object below
comment_tokens_l <- list()

for (com in 1:nrow(AllComments_df)) {
  sto <- NA
  sto <- AllComments_df$textOriginal[com]
  
  #@ preprocess sto  
  sto.list <- list()
  sto <- tolower(sto)
  sto.list <- strsplit(sto, "\\W")
  sto.text <- unlist(sto.list)
  sto_rm <- sto.text[-which(sto.text == "")]; sto.text <- sto_rm
  if(length (sto.text) >= 40 & length(unique(sto.text)) >=15 ) {
    #@ create list.head               
    list.head <- NA
    list.head <- AllComments_df$id[com]
    #list.head <- substring(list.head, 1, nchar(list.head)-20)  (subset the comment title string if/as desired)
    comment_tokens_l[[list.head]] <- sto.text
  
  }}
print(paste("keeping",length(comment_tokens_l), "out of", nrow(AllComments_df),"comments"))


#View(comment_tokens_l)
#View(AllComments_df)

list_names <- names(comment_tokens_l)

#head(list.names)
#View(list.data)
```

### Get Ready to Wordfish!
```{r data_reshape_for_WF, echo=FALSE, warning=FALSE, results='hide'}
##


  # "\n" for newline separator. This may or may not be needed depeing on your data
  # May try with or without the "sep" argument, or try feeding into "sep" other kinds of seperator


####### Turn the list of comment files into a corpus data object
####### this kind of analyisscan only be executed on a corpus object
comment_corpus <- Corpus(VectorSource(comment_tokens_l))   #@ this function needs a list of tokenized char vectors

#str(comment_corpus)
#comment_corpus[["1"]][["content"]]   # it can be good to try to visualize different aspects of your data objects

comment_corpus <- tm_map(comment_corpus, removeNumbers)
comment_corpus <- tm_map(comment_corpus, removePunctuation)
#?tm_map()

#my.stop <- stopwords("english")
my_stop <- c(stopwords("english"),
             "c","x", "s", "t", "m", "amp", "youtube", "www", "com", "quot", "br", "http", "https", "")

comment_corpus <- tm_map(comment_corpus, removeWords, my_stop)
beep("coin")


#!!!The DocumentTermMatrix
dtm = DocumentTermMatrix(comment_corpus)
#dtm

#?DocumentTermMatrix()
```


### Control Sparseness and Run the Wordfish model
```{r WF_model, echo=TRUE, results='hide'}
########################################
########## Set accepted sparsity parameter
########################################
dtma = removeSparseTerms(dtm, sparse = .998)
#@ including too many very sparse (infrequent) terms negatively impacts results
beep("coin")
#str(dtma)
print(paste("using",length(dtma$dimnames$Terms),"words from", 
            length(dtma$dimnames$Docs), "of", nrow(AllComments_df), "comments... trying to WF!"))


########################################
####### Running a wordfish model
########################################
wfa1 <- wordfish(as.wfm(dtma), dir=c(1, 2), control = list(tol = 3.0e-5), verbose = T)
# beeps are useful to signify when long processes have completed
beep("coin")
beep("complete")
```

### Store data from WF model
```{r store_WF_model_data, echo=FALSE}
#head(wfa1)

# Explore Wordfish --------------------------------------------------------

#@ these commands isolate the document-level data
wfa1$docs <- names(comment_tokens_l)
wfdocs.v <- names(comment_tokens_l)

wfdocs.v <- wfa1[["docs"]]
theta <- wfa1[["theta"]]
alpha <- wfa1[["alpha"]]

#@ the [-1] indexing for these removes the empty strings ("") that are sometimes an artifact of this method
#@ you SHOULD NOT need this
(head(wfa1$words))

wf.words.v <- wfa1[["words"]];    #wf.words.v <- wf.words.v[-1]
beta <- wfa1[["beta"]];           #beta<-beta[-1]
psi <- wfa1[["psi"]];             #psi <- psi [-1]
```

### View some properties of the WF model
```{r view_WF_model_properties, echo=TRUE}
sum(theta[which(theta>0)])
sum(theta[which(theta<0)])
mean(theta)
sum(theta)

### View a histogram of the distribution of each key variable
hist(theta, breaks=30)  # document polarity (refined iteratively)
hist(alpha, breaks=30)  # fixed effect for document length
hist(beta, breaks=50)   # word polarity (refined iteratively)
hist(psi, breaks=50)    # fixed effect for term (aka ~type~) frequency

```

### Make Composite Data Objects
```{r make_WF_data_objects, echo=FALSE, results='hide'}
######################### Make Composite data objects
#View(wfdocs.v)

#######################  SOURCE is the news surce the article comes from  +++ need to 'keep' title somehow
# length(wfdocs.v)
source <- NA
for (i in 1:length(wfdocs.v)){
  sto<-NA
  sto <- gsub("[0-9]", "", wfdocs.v[i])
  sto <- gsub(".txt", "", sto)
  sto <- gsub("article","",sto)
  sto <- gsub("[:.:]","",sto)
  source[i] <- sto
}
#View(head(source))


#######################  TITLE is the title with metadata stripped out - find something relevant
title<-NA
for (i in 1:length(wfdocs.v)){
  sto<-NA
  sto <- substr(wfdocs.v[i],1,12 )
  title[i] <- sto
}
#View(head(title))



### CREATE DOC DATA DATAFRAME
wf_docdata.df <- data.frame(wfdocs.v, source, title, theta, alpha)
#View(wf_docdata.df)


### CREATE WORD DATA DATAFRAME
wf_worddata.df <- data.frame(wf.words.v, beta, psi)
#View(wf_worddata.df)

```

### Visualizations Directory
```{r save_vis_in_directory, echo=FALSE, eval=FALSE}
##### move working directory to the 'visualizations' repository   #@ eval=F; ignoring this chunk for now

### create and set a visualization directory
vis.dir <- "/Visualizations/"
dir.create("Visualizations")
working.vis.dir <- paste(proj.dir, vis.dir, sep="" )
working.vis.dir
setwd(working.vis.dir)

subject.dir <- paste(subject.s, ".vis", "/", sep="")
dir.create(subject.dir)
setwd(paste(data.dir, vis.dir, subject.dir, sep=""))
getwd()
```


### Initial Visualizations
```{r initial_visualizations, echo=FALSE}


############ Plot the two estimated document parameters: THETA against ALPHA, incl. METADATA #########
#@ Alpha = document fixed effect (control for length)
#@ Theta = document "polarity" - alignment along as-identified latent principle component

###### Label by USER color by CLIQUE     can include color=source in aes
user_T_A_plot <- ggplot(data = wf_docdata.df, mapping = aes(x =theta, y = alpha, label = title)) + 
  geom_text(size = 1.2) + 
  labs(x = "Comment polarity: an optimized value (theta)", y = "Comment length: a fixed effect (alpha)") +
  #guides(size = "none", color = guide_legend("")) + theme(legend.position="bottom") +
  labs(title = "Comment Polarity in comment network cliques of one thread scraped from YouTube", 
                      subtitle="'CNN reporter presses Trump: You promised Mexico would pay for wall'") 
user_T_A_plot 


ggsave(paste(subject.s, "SUBSET_user_T_A_plot.pdf",sep=""), device="pdf")



###### Color by SOURCE  (this does not currently work)
#source_T_A_plot <- ggplot(data = wf_docdata.df, mapping = aes(x =theta, y = alpha, label = title, color=source))+ 
#  geom_text(size = .7) + 
#  labs(x = "Doc polarity: an optimized value (theta)", y = "Doc length: a fixed effect (alpha)") +
#  guides(size = "none", color = guide_legend("")) + theme(legend.position="bottom") +
#  labs(title = paste (subject.s, 
#                      " comments from ABC YouTube Videos:\n Article IDs plotted, shaded by comment thread source", #sep="")) 
##     __ 
#source_T_A_plot
#ggsave(paste(subject.s, "SUBSET_source_T_A_plot.pdf",sep=""), device="pdf")



###### COLORLESS
source_T_A_plot <- ggplot(data = wf_docdata.df, mapping = aes(x =theta, y = alpha, label = title)) + 
  geom_text(size = 1) + 
  labs(x = "Doc polarity: an optimized value (theta)", y = "Doc length: a fixed effect (alpha)") +
  #guides(size = "none", color = guide_legend("")) + theme(legend.position="bottom") +
  labs(title = paste (subject.s, 
                      " comments from ABC YouTube Videos:\n Article IDs plotted, shaded by comment thread source", sep="")) 
##     __ 
source_T_A_plot

#ggsave(paste(subject.s, "SUBSET_colorless_T_A_plot.pdf",sep=""), device="pdf")


######################## FIRST PLOT of two word parameters: BETA against PSI, basic black and white
#@***limiting by a critical max negitive psi value***
word_P_B_plot <- ggplot(data = wf_worddata.df, mapping = aes(x = beta, y = psi, label = wf.words.v)) + 
  geom_text(data=subset(wf_worddata.df, psi>-20), size = 0.755) + 
  labs(x = "Word polarity: an optimized value (beta)", y = "Word frequency: a fixed effect (psi)") +
  #guides(size = "none", color = guide_legend("")) + 
  labs(title = "Vocabulary Polarity in comment network cliques of one thread scraped from YouTube", 
       subtitle="'CNN reporter presses Trump: You promised Mexico would pay for wall'")
######           __
word_P_B_plot
getwd()

ggsave(paste(subject.s, "Word_P_B_plot.pdf",sep=""), device="pdf")

```

### Colorizing Key Words
```{r give_keywords_color, echo=FALSE}
######################## SO optimized!!
#wf_worddata.df_unreduced <- wf_worddata.df
## RESET wf_worddata.df if/when needed     __ wf_worddata.df <- data.frame(word, beta, psi)

######+++++++++++++++++++++++++++++++++++++++++++ Set which KEY TERMS should have BIG FONT
    
neutral <- "neutral"  # Grey
topA <- "Of Interest"      # Red
topB <- "Wall"      # 
topC <- "Money"      # 
topD <- "Trade"      # 
topE <- "Rape"     # 
topF <- "Voting"   # 
topG <- "Drugs"    # 
topH <- "Jobs"     # 
#topic_colors <- c()

wf_worddata.df$key <- neutral  ### SET / RESET default font SIZE/COLOR and word coding


ktA <- c("trump", "animal", "animals", "obama", "illegal", "mexico", "mexican", "caravan", 
         "jew", "jews", "jewish", "security", "secure", "national", "nation")
for(k in 1:length(ktA)){
  sto <- NA
  sto <- (which(word==ktA[k]))
  #print(sto)
  wf_worddata.df$key[sto] <-topA }   ### NEW FONT SIZE
#View(wf_worddata.df[which(wf_worddata.df$key==topA),])


ktB <- c("wall", "build", "steel", "concrete", "border", "dig", "tunnels", "bars", "invisible", "see")
for(k in 1:length(ktB)){
  sto <- NA
  sto <- (which(word==ktB[k]))
  wf_worddata.df$key[sto] <-topB }    ### NEW FONT SIZE
#View(wf_worddata.df[which(wf_worddata.df$key==topB),])

ktC <- c("money", "tax", "taxes", "taxpayers", "billion", "billions", "spend", "spent", "pay", "billionz")
for(k in 1:length(ktC)){
  sto <- NA
  sto <- (which(word==ktC[k]))
  wf_worddata.df$key[sto] <-topC }    ### NEW FONT SIZE
#View(wf_worddata.df[which(wf_worddata.df$key==topC),])

ktD <- c("nafta", "trade", "usmca")
for(k in 1:length(ktD)){
  sto <- NA
  sto <- (which(word==ktD[k]))
  wf_worddata.df$key[sto] <-topD }    ### NEW FONT SIZE
#View(wf_worddata.df[which(wf_worddata.df$key==topD),])


ktE <- c("rape", "raped", "raping", "rapes", "assault", "sexual", "victim", "rapists", "ptsd")
for(k in 1:length(ktE)){
  sto <- NA
  sto <- (which(word==ktE[k]))
  wf_worddata.df$key[sto] <-topE }    ### NEW FONT SIZE
#View(wf_worddata.df[which(wf_worddata.df$key==topE),])

ktF <- c("vote", "voting", "election", "elections", "electoral", "rights", "birthrights")
for(k in 1:length(ktF)){
  sto <- NA
  sto <- (which(word==ktF[k]))
  wf_worddata.df$key[sto] <-topF }    ### NEW FONT SIZE
#View(wf_worddata.df[which(wf_worddata.df$key==topF),])

ktG <- c("drug", "drugs", "illicit", "police", "marijuana", "heroin", "coke", "cocaine", "fentanyl")
for(k in 1:length(ktG)){
  sto <- NA
  sto <- (which(word==ktG[k]))
  wf_worddata.df$key[sto] <-topG }    ### NEW FONT SIZE
#View(wf_worddata.df[which(wf_worddata.df$key==topG),])


ktH <- c("job", "jobs", "economic", "work", "worker", "labor", "laborer", "farm", "farms", "agriculture")
for(k in 1:length(ktH)){
  sto <- NA
  sto <- (which(word==ktH[k]))
  wf_worddata.df$key[sto] <-topH }    ### NEW FONT SIZE
#View(wf_worddata.df[which(wf_worddata.df$key==topH),])

```

### Final Colorized-by-words plot
```{r word_color_vis, echo=FALSE, warning=FALSE}

#+++++++++++++++  use this step to snip off extreem beta and psi vales to maximize plotting area

#lowpsi <- which(wf_worddata.df$psi <= (-9.9))
#lowbeta <- which(wf_worddata.df$beta <= (-4.4))
#wf_worddata.df <- wf_worddata.df[(-lowpsi), ]
#wf_worddata.df <- wf_worddata.df[(-lowbeta), ]

#which(wf_worddata.df$psi <= (-13))    ## check your work!

wordLegend_P_B_plot <- ggplot(data = wf_worddata.df, mapping = aes(x = beta, y = psi, label = word, color=key)) + 
  ylim(-11,.2)+ xlim(-5,6) +       #tight limits
  #ylim(-14,.2)+ xlim(-7,8) +     #more expanded limits
  geom_text(data=subset(wf_worddata.df, key== neutral), size = .85, color="gray") + 
  geom_text(data=subset(wf_worddata.df, key!= neutral), size = 2.2) +
  scale_color_discrete(l=40) +
  #scale_color_manual(values=c("#00008B", "#8B2323", "#006400", "goldenrod4", "#8B0A50" )) +
  guides(size = "none", color = guide_legend("")) + theme(legend.position = "top") +
  labs(x = "Word polarity: an optimized value (beta)", y = "Word frequency: a fixed effect (psi)") +
  labs(title = "Vocabulary Polarity in comment network cliques of one thread scraped from YouTube", 
       subtitle="'CNN reporter presses Trump: You promised Mexico would pay for wall'\n key terms bolded")

######           __
wordLegend_P_B_plot

#ggsave(paste(subject.s, "_QQQQ_SUBSET_wordLegend_P_B_plot.pdf",sep=""), device="pdf")



```

### Final Beeps for All Done!
```{r final_beeps, echo=TRUE}
beep("coin");beep("coin")
beep("complete")
beep("mario")
```








~~~~~~~~~~~~~~~~~~~~~~~~~~
# HELPFUL MARKDWON SYNTAX REFERENCE SECTION 
### this is NOT for the final version - just leaving it in until we have the syntax down pat
(random extra sample code and whatnot)

`r 2+3`  
`2+3`

$A = pi*r^2$  
$$V = 1/3(pi*r^3)$$

```{r echo=TRUE}
# you NEED {r}!
## moar code
moar <- "moar code here"
print(moar)
vec <- c(1,2,3,4,5)
str.vec <- c("a", "b", "c")
```

```{r, echo=FALSE}
#doesn't work without {r}
#moar code
moarr <- "moar code again"
print(moarr)
```

### examples

`r moarr`  
```{r, collapse=FALSE, echo=TRUE, eval=FALSE}
print("this is a string")
```
`vec`  
`r c(vec, str.vec)`  
`r mean(c(1,2,3,4,5))`

text1  
text2  
text3
text3.5
### R Markdown tutorial

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.


# DON'T NEED THIS PROBABLY
### Directory Paths
I don't think we'll need this chunk.  Right now, it will be 'ignored' by the kint markdown report.
```{r create_directories, echo=F, results='hide', eval=F}
# Set Working Directory, get data (not currently needed, since data is scraped)  
#@ also, in markdown, you have to reset the directory in each chunk, if /as necessesary

input.dir <- "csv_Data/" #@ this folder contains .csv files of comment data (old skool)
proj.dir <- "~/Box Sync/Consulting/DSC_Youtube/"

#setwd(proj.dir)
getwd()
```

### Read-in data
I don't think we'll need this chunk either.  
```{r read_data, echo=F, results='hide', eval=F}
#files.v <- dir(input.dir, "\\.csv$")   # old version used .tab$
#head(files.v)

#?stringi()
#?head()

#length(files.v)
#files.v
```

### @Extra code here - word clouds and wordscore trials
```{r extra_code, echo=FALSE, eval=FALSE}
#
#
#
############### Trying out Wordclouds....?
############### Trying out Wordclouds....?
############### Trying out Wordclouds....?

#@ WC separate articles by 


###### Wordcloud!
cloud.dtm <- TermDocumentMatrix(news.corpus)
cloud.m <- as.matrix(cloud.dtm)
cloud.v <- sort(rowSums(cloud.m),decreasing=TRUE)
cloud.df <- data.frame(word = names(cloud.v),freq=cloud.v, stringsAsFactors = F)
cloud.df <- cloud.df[-1,]
View(head(cloud.df, 10))

### remove " " and , from words
clean.cloud.df <- cloud.df
cloud.df[5,]
word<-NA
for (i in 1:nrow(cloud.df)){
  sto<-NA
  #sto <- gsub("\\W", "", cloud.df[i,1])  # this approach isn't working
  sto <- cloud.df[i,1]
  sto <- substr(sto,2,(nchar(sto)-2))
  clean.cloud.df[i,1] <- sto
}
View(head(clean.cloud.df, 10))



set.seed(1234)
wordcloud(words = clean.cloud.df$word, freq = clean.cloud.df$freq, min.freq = 1,
          max.words=99, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))


findAssocs(cloud.dtm, terms = "dadt", corlimit = 0.3)  ## doesn't work, presumably b/c of quotes and '/', ',' issues

cloud.dtm
head(cloud.m)

############### Trying out Wordclouds....?
############### Trying out Wordclouds....?
############### Trying out Wordclouds....?
#
#
#
#
#



############### Trying out Wordscores++++++++++++++++++++++++++++++ SKIP WORDSCORE BIT FOR NOW!
############### Trying out Wordscores
############### Trying out Wordscores
############### Trying out Wordscores



#install.packages("quanteda")
require(quanteda)
#install.packages("quanteda.corpora")
#require(quanteda.corpora)

files.v[c(4,47)]
ref <- c(1,47) # reference texts
vir <- 1:length(files.v) # SPS 2011 (short) is empty, thus not included
vir <- vir[-ref] # everything minus the reference texts
ref; vir

#?getdocs()
#?wfm()
str(corpus)
news.as.corpus<- corpus(news.corpus)
str(news.as.corpus)
r <- getdocs(news.as.corpus, ref)


#?classic.wordscores()
ws <- classic.wordscores(r, scores=c(0,1.4))



################
############### Trying out Wordscores
############### Trying out Wordscores
############### Trying out Wordscores
############### Trying out Wordscores+++++++++++++++++++++++++++++++++++++++++ END WORDSCORE
#
#

```
